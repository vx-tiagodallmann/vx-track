"""
M√≥dulo ML Aprimorado com Modelo Treinado
Integra o modelo Random Forest treinado com dados hist√≥ricos
"""

import pickle
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import re
import unicodedata
from datetime import datetime
import os

class EnhancedMLAnalyzer:
    """Analisador ML aprimorado com modelo treinado em dados hist√≥ricos"""
    
    def __init__(self, model_path: str = '/home/ubuntu/best_ml_model.pkl'):
        """
        Inicializa o analisador com modelo treinado
        
        Args:
            model_path: Caminho para o arquivo do modelo treinado
        """
        self.model_path = model_path
        self.model = None
        self.model_name = None
        self.training_date = None
        self.confidence_threshold = 0.6
        
        # Carregar modelo se existir
        self.load_model()
        
        # Fallback para regras baseadas em palavras-chave
        self.keyword_rules = {
            'Configura√ß√£o': [
                'configura√ß√£o', 'configurar', 'config', 'parametrizar', 'parametros',
                'setup', 'instala√ß√£o', 'instalar', 'certificado', 'nfe', 'nfce',
                'fiscal', 'tributa√ß√£o', 'impostos', 'icms', 'ipi', 'pis', 'cofins'
            ],
            'Treinamento': [
                'treinamento', 'treinar', 'capacita√ß√£o', 'capacitar', 'ensinar',
                'explicar', 'demonstrar', 'orientar', 'instruir', 'curso',
                'apresenta√ß√£o', 'workshop', 'tutorial'
            ],
            'Suporte': [
                'suporte', 'ajuda', 'problema', 'erro', 'bug', 'falha',
                'd√∫vida', 'quest√£o', 'dificuldade', 'resolver', 'solucionar',
                'corrigir', 'debug', 'troubleshooting'
            ],
            'Implanta√ß√£o': [
                'implanta√ß√£o', 'implantar', 'implementa√ß√£o', 'implementar',
                'deploy', 'migra√ß√£o', 'migrar', 'convers√£o', 'converter',
                'go-live', 'golive', 'produ√ß√£o', 'ativa√ß√£o'
            ]
        }
    
    def load_model(self) -> bool:
        """
        Carrega o modelo treinado
        
        Returns:
            True se modelo foi carregado com sucesso, False caso contr√°rio
        """
        try:
            if os.path.exists(self.model_path):
                with open(self.model_path, 'rb') as f:
                    model_data = pickle.load(f)
                
                self.model = model_data['model']
                self.model_name = model_data['model_name']
                self.training_date = model_data.get('training_date')
                
                print(f"‚úÖ Modelo carregado: {self.model_name}")
                print(f"üìÖ Data de treinamento: {self.training_date}")
                return True
            else:
                print(f"‚ö†Ô∏è Modelo n√£o encontrado em: {self.model_path}")
                print("üîÑ Usando classifica√ß√£o baseada em regras como fallback")
                return False
                
        except Exception as e:
            print(f"‚ùå Erro ao carregar modelo: {str(e)}")
            print("üîÑ Usando classifica√ß√£o baseada em regras como fallback")
            return False
    
    def preprocess_text(self, text: str) -> str:
        """
        Pr√©-processa texto da mesma forma que no treinamento
        
        Args:
            text: Texto a ser processado
            
        Returns:
            Texto pr√©-processado
        """
        if not text or pd.isna(text):
            return ""
        
        # Converter para string e min√∫sculas
        text = str(text).lower()
        
        # Remover acentos
        text = unicodedata.normalize('NFD', text)
        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')
        
        # Remover caracteres especiais, manter apenas letras, n√∫meros e espa√ßos
        text = re.sub(r'[^a-z0-9\s]', ' ', text)
        
        # Remover espa√ßos m√∫ltiplos
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def classify_with_model(self, text: str) -> Tuple[str, float]:
        """
        Classifica texto usando modelo treinado
        
        Args:
            text: Texto a ser classificado
            
        Returns:
            Tupla (tipo_servico, confianca)
        """
        if not self.model:
            return self.classify_with_rules(text)
        
        try:
            # Pr√©-processar texto
            processed_text = self.preprocess_text(text)
            
            if not processed_text:
                return "Outros", 0.0
            
            # Fazer predi√ß√£o
            prediction = self.model.predict([processed_text])[0]
            
            # Obter probabilidades se dispon√≠vel
            if hasattr(self.model, 'predict_proba'):
                probabilities = self.model.predict_proba([processed_text])[0]
                confidence = max(probabilities)
                
                # Se confian√ßa muito baixa, usar regras como fallback
                if confidence < self.confidence_threshold:
                    rule_type, rule_conf = self.classify_with_rules(text)
                    if rule_conf > 0.5:  # Se regras t√™m boa confian√ßa
                        return rule_type, rule_conf
            else:
                confidence = 0.8  # Confian√ßa padr√£o para modelos sem probabilidade
            
            return prediction, confidence
            
        except Exception as e:
            print(f"‚ùå Erro na classifica√ß√£o com modelo: {str(e)}")
            return self.classify_with_rules(text)
    
    def classify_with_rules(self, text: str) -> Tuple[str, float]:
        """
        Classifica texto usando regras baseadas em palavras-chave (fallback)
        
        Args:
            text: Texto a ser classificado
            
        Returns:
            Tupla (tipo_servico, confianca)
        """
        if not text or pd.isna(text):
            return "Outros", 0.0
        
        text_lower = text.lower()
        scores = {}
        
        # Calcular score para cada categoria
        for category, keywords in self.keyword_rules.items():
            score = 0
            for keyword in keywords:
                if keyword in text_lower:
                    score += 1
            
            if score > 0:
                # Normalizar score pelo n√∫mero de palavras-chave da categoria
                scores[category] = score / len(keywords)
        
        if not scores:
            return "Outros", 0.3
        
        # Retornar categoria com maior score
        best_category = max(scores, key=scores.get)
        confidence = min(scores[best_category] * 2, 1.0)  # Normalizar para [0,1]
        
        return best_category, confidence
    
    def extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """
        Extrai palavras-chave relevantes do texto
        
        Args:
            text: Texto para extra√ß√£o
            max_keywords: N√∫mero m√°ximo de palavras-chave
            
        Returns:
            Lista de palavras-chave
        """
        if not text or pd.isna(text):
            return []
        
        # Pr√©-processar texto
        processed_text = self.preprocess_text(text)
        
        if not processed_text:
            return []
        
        # Palavras comuns a ignorar
        stop_words = {
            'de', 'da', 'do', 'das', 'dos', 'a', 'o', 'as', 'os', 'e', 'ou',
            'para', 'por', 'com', 'em', 'no', 'na', 'nos', 'nas', 'um', 'uma',
            'uns', 'umas', 'foi', 'ser', 'ter', 'que', 'se', 'ao', 'aos',
            'realizada', 'realizado', 'feito', 'feita', 'executado', 'executada'
        }
        
        # Extrair palavras
        words = processed_text.split()
        
        # Filtrar palavras relevantes
        keywords = []
        for word in words:
            if (len(word) >= 3 and 
                word not in stop_words and 
                not word.isdigit()):
                keywords.append(word)
        
        # Remover duplicatas mantendo ordem
        unique_keywords = []
        seen = set()
        for keyword in keywords:
            if keyword not in seen:
                unique_keywords.append(keyword)
                seen.add(keyword)
        
        return unique_keywords[:max_keywords]
    
    def generate_summary(self, text: str, max_length: int = 100) -> str:
        """
        Gera resumo do texto
        
        Args:
            text: Texto para resumir
            max_length: Comprimento m√°ximo do resumo
            
        Returns:
            Resumo do texto
        """
        if not text or pd.isna(text):
            return ""
        
        # Se texto j√° √© curto, retornar como est√°
        if len(text) <= max_length:
            return text
        
        # Dividir em senten√ßas
        sentences = re.split(r'[.!?]+', text)
        
        if not sentences:
            return text[:max_length] + "..."
        
        # Pegar primeira senten√ßa completa que caiba no limite
        summary = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence:
                if len(summary + sentence) <= max_length:
                    summary += sentence + ". "
                else:
                    break
        
        # Se nenhuma senten√ßa coube, truncar primeira senten√ßa
        if not summary and sentences[0]:
            summary = sentences[0].strip()[:max_length-3] + "..."
        
        return summary.strip()
    
    def analyze_text(self, text: str, include_ml_analysis: bool = True) -> Dict[str, Any]:
        """
        An√°lise completa do texto
        
        Args:
            text: Texto a ser analisado
            include_ml_analysis: Se deve incluir an√°lise ML
            
        Returns:
            Dicion√°rio com resultados da an√°lise
        """
        if not text or pd.isna(text):
            return {
                'tipo_servico': 'Outros',
                'confianca': 0.0,
                'palavras_chave': [],
                'resumo': '',
                'modelo_usado': 'Nenhum',
                'timestamp': datetime.now().isoformat()
            }
        
        # Classifica√ß√£o
        if include_ml_analysis:
            tipo_servico, confianca = self.classify_with_model(text)
            modelo_usado = self.model_name if self.model else 'Regras'
        else:
            tipo_servico, confianca = self.classify_with_rules(text)
            modelo_usado = 'Regras'
        
        # Extra√ß√£o de palavras-chave
        palavras_chave = self.extract_keywords(text)
        
        # Gera√ß√£o de resumo
        resumo = self.generate_summary(text)
        
        return {
            'tipo_servico': tipo_servico,
            'confianca': round(confianca, 3),
            'palavras_chave': palavras_chave,
            'resumo': resumo,
            'modelo_usado': modelo_usado,
            'timestamp': datetime.now().isoformat(),
            'texto_original': text,
            'comprimento_texto': len(text)
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        Retorna informa√ß√µes sobre o modelo carregado
        
        Returns:
            Dicion√°rio com informa√ß√µes do modelo
        """
        return {
            'modelo_carregado': self.model is not None,
            'nome_modelo': self.model_name,
            'data_treinamento': self.training_date,
            'caminho_modelo': self.model_path,
            'threshold_confianca': self.confidence_threshold,
            'fallback_disponivel': True
        }
    
    def update_confidence_threshold(self, new_threshold: float) -> None:
        """
        Atualiza o threshold de confian√ßa
        
        Args:
            new_threshold: Novo threshold (0.0 a 1.0)
        """
        if 0.0 <= new_threshold <= 1.0:
            self.confidence_threshold = new_threshold
            print(f"‚úÖ Threshold de confian√ßa atualizado para: {new_threshold}")
        else:
            print("‚ùå Threshold deve estar entre 0.0 e 1.0")


# Inst√¢ncia global para uso na aplica√ß√£o
enhanced_analyzer = EnhancedMLAnalyzer()


def analyze_service_description(text: str, **kwargs) -> Dict[str, Any]:
    """
    Fun√ß√£o de conveni√™ncia para an√°lise de descri√ß√£o de servi√ßo
    Mant√©m compatibilidade com vers√£o anterior
    
    Args:
        text: Texto a ser analisado
        **kwargs: Argumentos adicionais
        
    Returns:
        Resultado da an√°lise
    """
    return enhanced_analyzer.analyze_text(text, **kwargs)


def get_enhanced_ml_info() -> Dict[str, Any]:
    """
    Retorna informa√ß√µes sobre o ML aprimorado
    
    Returns:
        Informa√ß√µes do sistema ML
    """
    return enhanced_analyzer.get_model_info()


# Fun√ß√£o para teste
def test_enhanced_ml():
    """Testa o ML aprimorado com exemplos"""
    test_cases = [
        "Configura√ß√£o completa do m√≥dulo NFe incluindo certificado digital",
        "Treinamento dos usu√°rios sobre emiss√£o de notas fiscais",
        "Suporte para resolver erro na transmiss√£o de NFCe",
        "Implanta√ß√£o do sistema ERP na filial de S√£o Paulo",
        "An√°lise de relat√≥rios gerenciais e indicadores"
    ]
    
    print("üß™ Testando ML Aprimorado:")
    print("=" * 50)
    
    for i, text in enumerate(test_cases, 1):
        result = enhanced_analyzer.analyze_text(text)
        print(f"\n{i}. Texto: {text}")
        print(f"   Tipo: {result['tipo_servico']} ({result['confianca']:.1%})")
        print(f"   Palavras-chave: {', '.join(result['palavras_chave'])}")
        print(f"   Modelo: {result['modelo_usado']}")


if __name__ == "__main__":
    test_enhanced_ml()

